\section{Introduction}
\subsection{Problem Setup}
Suppose $X \in \mathbb{R}^{n}$ and $Z \in \mathbb{R}^{m}$ with $X=f(Z)+\varepsilon$, where $f: \mathbb{R}^{m} \rightarrow \mathbb{R}^{n}$ and $\varepsilon \sim N\left(0, \sigma^{2} I\right)$. Assuming $Z$ is normally distributed with independent marginals, this is equivalent to the following latent variable model (a special case of the well-known \textit{nonlinear ICA} model):
\begin{equation*}
\begin{aligned}
Z & \sim N\left(0, I\right) \\
X \mid Z & \sim N\left(f(Z), \sigma^{2} I\right) .
\end{aligned}
\end{equation*}

Let $\varphi(u ; \mu, \Sigma)$ denote the density of a $N(\mu, \Sigma)$ random variable and $p_{\theta, \sigma^{2}}(x, z)$ denote the joint density under the model. It is easy to see that
\begin{equation*}
\begin{aligned}
p_{\theta, \sigma^{2}}\left(x, z\right) &=p_{\theta, \sigma^{2}} \left(x \mid z\right) p(z)=\varphi\left(x ; f(z), \sigma^{2} I\right) \varphi(z ; 0, I) \\
L \left(\theta, \sigma^{2} ; x\right) &= p_{\theta, \sigma^{2}}\left(x\right) =\int \varphi\left(x ; f(z), \sigma^{2} I\right) \varphi(z ; 0, I) dz
\end{aligned}
\end{equation*}

\subsection{Objective Function}
Now, suppose we let $g_{\theta}$ denote a family of deep neural network distributions parametrized by $\theta$. To approximate the marginal density $p(x)$, we replace $f$ with $g_{\theta}$ and try to find the choice of $\theta$ that maximizes the observed data likelihood. Given $k$ observations $x^{(i)} \stackrel{i.i.d}{\sim} p(x)$, we wish to solve the following maximum likelihood problem:
\begin{equation*}
\max_{\theta, \sigma^{2}} \underbrace{\sum_{i=1}^{k} \log \int \varphi\left(x^{(i)} ; g_{\theta}(z), \sigma^{2} I\right) \varphi(z ; 0, I) dz}_{:=\ell(\theta, \sigma^{2})}
\end{equation*}

\subsection{Nonlinear ICA}
Now we show how the model above is closely related to previous work on nonlinear ICA. In nonlinear ICA, we assume observations $x \in \mathbb{R}^{d}$, which are the result of an unknown (but invertible) transformation $f$ of latent variables $z \in \mathbb{R}^{d}$:
\begin{equation*}
x=f(z)
\end{equation*}
where $z$ are assumed to follow a factorized (but typically unknown) distribution $p(z)=\prod_{i=1}^{d} p_{i} \left(z_{i}\right)$. This model is essentially a deep generative model.

The difference to the definition above is mainly in the lack of noise and the equality of the dimensions: the transformation $f$ is deterministic and invertible. Thus, any posteriors would be degenerate. The goal is then to recover (identify) $f^{-1}$, which gives the independent components as $z=f^{-1}(x)$, based on a dataset of observations of $x$ alone.
