\section{Variational Method}
In this section, we consider variational inference and VAEs. We use the ELBO to obtain a lower bound on the likelihood $\ell(\theta, \sigma)$ and optimize the ELBO using SGD. The marginal likelihoods of individual datapoints can each be rewritten as
\begin{equation*}
\log p_{\theta}\left(x^{(i)}\right)=D_{K L}\left(q_{\phi}\left(z \mid x^{(i)}\right) \| p_{\theta}\left(z \mid x^{(i)} \right)\right)+ \mathcal{L}\left(\theta, \phi ; x^{(i)} \right)
\end{equation*}

The second RHS term $\mathcal{L} \left(\theta, \phi ; x^{(i)} \right)$ is called the evidence lower bound on the marginal likelihood of datapoint $i$, and can be written as
\begin{equation*}
\begin{aligned}
\log p_{\theta}\left(x^{(i)}\right) \geq \mathcal{L}\left(\theta, \phi ;x^{(i)}\right)&=\mathbb{E}_{q_{\phi}(z \mid x)}\left[-\log q_{\phi}(z \mid x)+\log p_{\theta}(x, z)\right] \\
&=-D_{KL}\left(q_{\boldsymbol{\phi}}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right) \| p_{\boldsymbol{\theta}}(\mathbf{z})\right)+\mathbb{E}_{q_{\boldsymbol{\phi}}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right)}\left[\log p_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)} \mid \mathbf{z}\right)\right]
\end{aligned}
\end{equation*}

We want to differentiate and optimize the lower bound $\mathcal{L}\left(\boldsymbol{\theta}, \phi ; \mathbf{x}^{(i)}\right)$ w.r.t. both the variational parameters $\phi$ and generative parameters $\theta$.
